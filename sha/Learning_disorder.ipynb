{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the Dataset\n",
    "df = pd.read_csv('data_csv.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas Profiling, generate a data profiling report for a Pandas DataFrame and then save that report to an HTML file.\n",
    "from ydata_profiling import ProfileReport\n",
    "prof = ProfileReport(df)\n",
    "prof.to_file(output_file = 'ASD.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function called save_fig for saving figures or plots generated in your data preprocessing or analysis\n",
    "# function is designed to save the figures as image files in a specific directory.\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "IMAGES_PATH = Path() / \"images\" / \"Data Preprocessing\"\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# extra code â€“ the next 5 lines define the default font sizes\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "\n",
    "df.hist(bins=50, figsize=(12, 8))\n",
    "save_fig(\"attribute_histogram_plots\")  # extra code\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical data into numerical form\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Define the categorical columns you want to label encode\n",
    "categorical_columns = ['Speech Delay/Language Disorder', 'Learning disorder', 'Genetic_Disorders', 'Depression', \n",
    "                       'Global developmental delay/intellectual disability',\n",
    "                       'Social/Behavioural Issues', 'Childhood Autism Rating Scale', 'Anxiety_disorder', \n",
    "                       'Sex', 'Ethnicity', 'Jaundice', 'Family_mem_with_ASD', 'Who_completed_the_test', 'ASD_traits']\n",
    "\n",
    "for column in categorical_columns:\n",
    "    labelencoder = LabelEncoder()\n",
    "    \n",
    "    df[column] = labelencoder.fit_transform(df[column])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_opts = dict(method='zip',\n",
    "                        archive_name='preprocessed.csv')  \n",
    "df.to_csv('out.zip', index=False,\n",
    "          compression=compression_opts)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "folder_path = Path('C:/Shanila/CSE/cse445/final_project/sha')\n",
    "filepath = folder_path / 'preprocessed.csv'\n",
    "\n",
    "folder_path.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> import os  \n",
    ">>> os.makedirs('folder/subfolder', exist_ok=True)  \n",
    ">>> df.to_csv('folder/subfolder/out.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = 'C:/Shanila/CSE/cse445/final_project/sha'\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "df.to_csv(os.path.join(folder_path, 'preprocessed.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"Learning disorder\", \"CASE_NO_PATIENT'S\"], axis=1)\n",
    "y= df['Learning disorder']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seperate train 80 and test 20 set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save train and testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Specify the folder paths\n",
    "train_folder = \"C:\\\\Shanila\\\\CSE\\\\cse445\\\\final_project\\\\sha\\\\train\"\n",
    "test_folder = \"C:\\\\Shanila\\\\CSE\\\\cse445\\\\final_project\\\\sha\\\\test\"\n",
    "\n",
    "# Create the folders if they don't exist\n",
    "os.makedirs(train_folder, exist_ok=True)\n",
    "os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "# Save X_train and y_train as CSV files in the \"train\" folder\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "train_data.to_csv(os.path.join(train_folder, \"train_data.csv\"), index=False)\n",
    "\n",
    "# Save X_test and y_test as CSV files in the \"test\" folder\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "test_data.to_csv(os.path.join(test_folder, \"test_data.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing ML Algorithms to test\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Results(clf):\n",
    "    print(clf)\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    print('Train Accuracy', accuracy_score(y_train, clf.predict(X_train)))\n",
    "    print('Test Accuracy', accuracy_score(y_test, predictions))\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    print(classification_report(y_test, predictions))\n",
    "    #also pretty confusion matrix\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn2 = KNeighborsClassifier(n_neighbors = 15)\n",
    "dt1 = DecisionTreeClassifier() #Gini\n",
    "dt2 = DecisionTreeClassifier(criterion = 'entropy') #entropy\n",
    "rf = RandomForestClassifier()\n",
    "dummy = DummyClassifier(strategy = \"most_frequent\")\n",
    "dummy.fit(X, y)\n",
    "logr = LogisticRegression()\n",
    "\n",
    "classifiers = [dummy,  dt1, dt2, rf, knn, knn2, logr]\n",
    "\n",
    "for clf in classifiers:\n",
    "    Results(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save result in result seperately\n",
    "also has csv, doc and images png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from docx import Document\n",
    "from docx.enum.table import WD_ALIGN_VERTICAL\n",
    "from docx.shared import Pt\n",
    "from docx.oxml.ns import qn\n",
    "from docx.oxml import OxmlElement\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to save the results to both a CSV and a DOCX file\n",
    "def save_results_to_files(clf, csv_file_path, docx_file_path):\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    train_accuracy = accuracy_score(y_train, clf.predict(X_train))\n",
    "    test_accuracy = accuracy_score(y_test, predictions)\n",
    "    confusion = confusion_matrix(y_test, predictions)\n",
    "    report = classification_report(y_test, predictions, output_dict=True)\n",
    "\n",
    "    # Save results to CSV file\n",
    "    with open(csv_file_path, 'w') as file:\n",
    "        file.write(f\"Train Accuracy: {train_accuracy*100:.4f}%\\n\")\n",
    "        file.write(f\"Test Accuracy: {test_accuracy*100:.4f}%\\n\")\n",
    "        file.write(\"Confusion Matrix:\\n\")\n",
    "        file.write(str(confusion) + \"\\n\")\n",
    "        file.write(\"Classification Report:\\n\")\n",
    "\n",
    "        # Split the classification report into lines\n",
    "        report_lines = classification_report(y_test, predictions).split('\\n')\n",
    "        for line in report_lines:\n",
    "            # Check if the line is not empty\n",
    "            if line:\n",
    "                # Split the line into words\n",
    "                words = line.split()\n",
    "                # Format precision, recall, f1-score, and support as percentages\n",
    "                formatted_line = ' '.join([f'{float(word)*100:.4f}%' if '%' in word else word for word in words])\n",
    "                file.write(formatted_line + '\\n')\n",
    "\n",
    "    # Save results to DOCX file\n",
    "    document = Document()\n",
    "    document.add_heading('Classifier Results', 0)\n",
    "\n",
    "    # Create a table for the results\n",
    "    table = document.add_table(rows=1, cols=2)\n",
    "    table.style = 'Table Grid'\n",
    "\n",
    "    # Add header row to the table\n",
    "    header_cells = table.rows[0].cells\n",
    "    header_cells[0].text = 'Metric'\n",
    "    header_cells[1].text = 'Value'\n",
    "\n",
    "    # Add rows to the table\n",
    "    rows = [('Train Accuracy', f'{train_accuracy*100:.4f}%'),\n",
    "            ('Test Accuracy', f'{test_accuracy*100:.4f}%'),\n",
    "            ('Confusion Matrix', classification_report(y_test, predictions)),\n",
    "            ('Classification Report', classification_report(y_test, predictions))]\n",
    "\n",
    "    for metric, value in rows:\n",
    "        row_cells = table.add_row().cells\n",
    "        row_cells[0].text = metric\n",
    "        row_cells[1].text = value\n",
    "\n",
    "    # Save the DOCX file\n",
    "    document.save(docx_file_path)\n",
    "\n",
    "    # Save the confusion matrix as a PNG\n",
    "    save_confusion_matrix_as_png(confusion, clf)\n",
    "\n",
    "# Function to save the confusion matrix as a PNG image\n",
    "def save_confusion_matrix_as_png(matrix, clf):\n",
    "    # Create a custom color map with specified colors\n",
    "    colors = ['#B1BCE6', '#B2C8DF', '#C4D7E0', '#EFEFEF']\n",
    "    cmap = LinearSegmentedColormap.from_list('Custom', colors, N=matrix.max() + 1)\n",
    "\n",
    "    # Plot the confusion matrix with the custom color map\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(matrix, interpolation='nearest', cmap=cmap)\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Add labels to the confusion matrix\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            plt.text(j, i, matrix[i, j], horizontalalignment='center', verticalalignment='center', fontsize=12, color='black')\n",
    "\n",
    "    plt.title(f'Confusion Matrix - {clf}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "\n",
    "    # Save the confusion matrix as a PNG image\n",
    "    plt.savefig(f\"{result_directory}/{clf}_confusion_matrix.png\", bbox_inches='tight', dpi=100)\n",
    "    plt.close()\n",
    "\n",
    "# Specify the directory for saving the result files\n",
    "result_directory = \"C:\\\\Shanila\\\\CSE\\\\cse445\\\\final_project\\\\sha\\\\result\\\\\"\n",
    "\n",
    "# List of classifiers to evaluate\n",
    "classifiers = [dummy, dt1, dt2, rf, knn, knn2, logr]\n",
    "\n",
    "for clf in classifiers:\n",
    "    # Generate file names based on the classifier's name\n",
    "    classifier_name = clf.__class__.__name__\n",
    "    csv_file_path = result_directory + f\"{classifier_name}_results.csv\"\n",
    "    docx_file_path = result_directory + f\"{classifier_name}_results.docx\"\n",
    "\n",
    "    # Call the function to save the results\n",
    "    save_results_to_files(clf, csv_file_path, docx_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bar chart for test and train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# List of classifiers\n",
    "classifiers = [dummy, dt1, dt2, rf, knn, knn2, logr]\n",
    "\n",
    "# Train and test accuracies for each classifier\n",
    "train_accuracies = [accuracy_score(y_train, clf.predict(X_train)) for clf in classifiers]\n",
    "test_accuracies = [accuracy_score(y_test, clf.predict(X_test)) for clf in classifiers]\n",
    "\n",
    "# Bar positions\n",
    "positions = np.arange(len(classifiers))\n",
    "\n",
    "# Bar height\n",
    "bar_height = 0.35\n",
    "\n",
    "# Bar colors\n",
    "train_colors = ['#7895B2'] * len(classifiers)\n",
    "test_colors = ['#D2DAFF'] * len(classifiers)\n",
    "\n",
    "# Create the horizontal bar chart for train set accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(positions - bar_height/2, train_accuracies, bar_height, label='Train Set Accuracy', color=train_colors, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "# Create the horizontal bar chart for test set accuracy\n",
    "plt.barh(positions + bar_height/2, test_accuracies, bar_height, label='Test Set Accuracy', color=test_colors, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "# Add accuracy percentages on top of each bar\n",
    "for i in range(len(classifiers)):\n",
    "    plt.text(train_accuracies[i] + 0.005, i - bar_height / 2, f'{train_accuracies[i]*100:.2f}%', color='black', ha='left')\n",
    "    plt.text(test_accuracies[i] + 0.005, i + bar_height / 2, f'{test_accuracies[i]*100:.2f}%', color='black', ha='left')\n",
    "\n",
    "# Set the y-axis labels\n",
    "plt.yticks(positions, ['Dummy', 'Decision Tree (Gini)', 'Decision Tree (Entropy)', 'Random Forest', 'K-Nearest Neighbors (k=5)', 'K-Nearest Neighbors (k=15)', 'Logistic Regression'])\n",
    "\n",
    "# Set the x-axis label\n",
    "plt.xlabel('Accuracy')\n",
    "\n",
    "# Set the chart title\n",
    "plt.title('Classifier Train and Test Set Accuracy')\n",
    "\n",
    "# Set x-axis limits to provide more space for the labels\n",
    "plt.xlim(0, 1.3)\n",
    "\n",
    "# Show the legend\n",
    "plt.legend(loc='best')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot to the specified destination in high definition\n",
    "plot_save_path = \"C:\\\\Shanila\\\\CSE\\\\cse445\\\\final_project\\\\sha\\\\result2\\\\classifier_accuracy_horizontal.png\"\n",
    "plt.savefig(plot_save_path, bbox_inches='tight', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the result in results.csv FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Load the training and testing data from CSV files\n",
    "train_data = pd.read_csv(\"C:\\\\Shanila\\\\CSE\\\\cse445\\\\final_project\\\\sha\\\\train\\\\train_data.csv\")\n",
    "test_data = pd.read_csv(\"C:\\\\Shanila\\\\CSE\\\\cse445\\\\final_project\\\\sha\\\\test\\\\test_data.csv\")\n",
    "\n",
    "# Separate features (X) and target (y) for both training and testing data\n",
    "X_train = train_data.drop(columns=[\"Learning disorder\"])\n",
    "y_train = train_data[\"Learning disorder\"]\n",
    "X_test = test_data.drop(columns=[\"Learning disorder\"])\n",
    "y_test = test_data[\"Learning disorder\"]\n",
    "\n",
    "# Create a dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Initialize and evaluate different classifiers\n",
    "classifiers = {\n",
    "    \"Dummy Classifier\": DummyClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Logistic Regression\": LogisticRegression()\n",
    "}\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    # Fit the classifier on the training data\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the testing data\n",
    "    predictions = classifier.predict(X_test)\n",
    "    \n",
    "    # Evaluate the classifier and store the results\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    report = classification_report(y_test, predictions)\n",
    "    matrix = confusion_matrix(y_test, predictions)\n",
    "    \n",
    "    results[name] = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Classification Report\": report,\n",
    "        \"Confusion Matrix\": matrix\n",
    "    }\n",
    "\n",
    "# Create a DataFrame from the results dictionary\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Specify the path to save the results as a CSV file\n",
    "results_csv_path = \"C:\\\\Shanila\\\\CSE\\\\cse445\\\\final_project\\\\sha\\\\results.csv\"\n",
    "\n",
    "# Save the results as a CSV file\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "\n",
    "# Optionally, you can also display the results DataFrame\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYPERPARAMETER TUNING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Manual Hyperparameter Tuning\n",
    "model=RandomForestClassifier(n_estimators=300,criterion='entropy',\n",
    "                             max_features='sqrt',min_samples_leaf=10,random_state=100).fit(X_train,y_train)\n",
    "predictions=model.predict(X_test)\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(accuracy_score(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Manual Hyperparameter Tuning 2\n",
    "model=RandomForestClassifier(n_estimators=500,criterion='gini',\n",
    "                             max_features='sqrt',min_samples_leaf=10,random_state=100).fit(X_train,y_train)\n",
    "predictions=model.predict(X_test)\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(accuracy_score(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt','log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 1000,10)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10,14]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4,6,8]\n",
    "# Create the random grid\n",
    "# num of features 26\n",
    "random_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_features': ['auto', 'sqrt', 'log2'] + list(range(1, 27)) + [0.1, 0.2, 0.3, None],\n",
    "    'max_depth': max_depth,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'criterion': ['entropy', 'gini']\n",
    "}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " performing hyperparameter tuning for a Random Forest classifier using Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier()\n",
    "rf_randomcv=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=100,cv=3,verbose=2,\n",
    "                               random_state=100,n_jobs=-1)\n",
    "### fit the randomized model\n",
    "rf_randomcv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparison between entropy and gini graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Extract relevant data for Gini and Entropy configurations\n",
    "gini_mask = results['param_criterion'] == 'gini'\n",
    "entropy_mask = results['param_criterion'] == 'entropy'\n",
    "\n",
    "gini_results = {\n",
    "    'n_estimators': np.array(results['param_n_estimators'])[gini_mask],\n",
    "    'mean_test_score': np.array(results['mean_test_score'])[gini_mask]\n",
    "}\n",
    "\n",
    "entropy_results = {\n",
    "    'n_estimators': np.array(results['param_n_estimators'])[entropy_mask],\n",
    "    'mean_test_score': np.array(results['mean_test_score'])[entropy_mask]\n",
    "}\n",
    "\n",
    "# Create a figure and axes for the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot the comparison\n",
    "sns.lineplot(x=gini_results['n_estimators'], y=gini_results['mean_test_score'], label='Gini')\n",
    "sns.lineplot(x=entropy_results['n_estimators'], y=entropy_results['mean_test_score'], label='Entropy')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Comparison of Gini vs. Entropy in Randomized Search')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Mean Test Score')\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "save_path = r'C:\\Shanila\\CSE\\cse445\\final_project\\sha\\result3\\comparison_plot.png'\n",
    "plt.savefig(save_path, bbox_inches='tight')\n",
    "\n",
    "# Show the plot (optional)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seperate gini and entropy graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Extract relevant data for Gini and Entropy configurations\n",
    "gini_mask = results['param_criterion'] == 'gini'\n",
    "entropy_mask = results['param_criterion'] == 'entropy'\n",
    "\n",
    "gini_results = {\n",
    "    'n_estimators': np.array(results['param_n_estimators'])[gini_mask],\n",
    "    'mean_test_score': np.array(results['mean_test_score'])[gini_mask]\n",
    "}\n",
    "\n",
    "entropy_results = {\n",
    "    'n_estimators': np.array(results['param_n_estimators'])[entropy_mask],\n",
    "    'mean_test_score': np.array(results['mean_test_score'])[entropy_mask]\n",
    "}\n",
    "\n",
    "# Create a figure and axes for the Gini plot\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot the Gini results in blue\n",
    "sns.lineplot(x=gini_results['n_estimators'], y=gini_results['mean_test_score'], label='Gini', ax=ax1, color='blue')\n",
    "\n",
    "# Customize the Gini plot\n",
    "ax1.set_title('Randomized Search')\n",
    "ax1.set_xlabel('Number of Estimators')\n",
    "ax1.set_ylabel('Mean Test Score')\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "# Create a figure and axes for the Entropy plot\n",
    "fig, ax2 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot the Entropy results in orange\n",
    "sns.lineplot(x=entropy_results['n_estimators'], y=entropy_results['mean_test_score'], label='Entropy', ax=ax2, color='orange')\n",
    "\n",
    "# Customize the Entropy plot\n",
    "ax2.set_title('Randomized Search')\n",
    "ax2.set_xlabel('Number of Estimators')\n",
    "ax2.set_ylabel('Mean Test Score')\n",
    "ax2.legend(loc='upper left')\n",
    "\n",
    "# Save the Gini and Entropy plots as PNG files\n",
    "save_path_gini = r'C:\\Shanila\\CSE\\cse445\\final_project\\sha\\result3\\gini_plot.png'\n",
    "save_path_entropy = r'C:\\Shanila\\CSE\\cse445\\final_project\\sha\\result3\\entropy_plot.png'\n",
    "plt.savefig(save_path_gini, bbox_inches='tight')\n",
    "plt.savefig(save_path_entropy, bbox_inches='tight')\n",
    "\n",
    "# Show the plots (optional)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define your parameter grid\n",
    "n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "max_depth = [int(x) for x in np.linspace(10, 1000, 10)]\n",
    "min_samples_split = [2, 5, 10, 14]\n",
    "min_samples_leaf = [1, 2, 4, 6, 8]\n",
    "\n",
    "random_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_features': ['auto', 'sqrt', 'log2'] + list(range(1, 27)) + [0.1, 0.2, 0.3, None],\n",
    "    'max_depth': max_depth,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'criterion': ['entropy', 'gini']\n",
    "}\n",
    "\n",
    "# Create your RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Create RandomizedSearchCV\n",
    "rf_randomcv = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=random_grid,\n",
    "    n_iter=100,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    random_state=100,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the randomized model\n",
    "rf_randomcv.fit(X_train, y_train)\n",
    "\n",
    "# Access the results\n",
    "results = rf_randomcv.cv_results_\n",
    "\n",
    "# Print or use the 'results' variable for visualization\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rf_randomcv.cv_results_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_randomcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_randomcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_randomcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random_grid=rf_randomcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "best_random_grid.fit(X_train, y_train)\n",
    "y_pred=best_random_grid.predict(X_test)\n",
    "# print(confusion_matrix(y_test,y_pred))\n",
    "print(\"Accuracy Score {}\".format(accuracy_score(y_test,y_pred)))\n",
    "print(\"Classification report: {}\".format(classification_report(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save random search model best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "best_random_grid.fit(X_train, y_train)\n",
    "y_pred = best_random_grid.predict(X_test)\n",
    "\n",
    "accuracy_score_model = accuracy_score(y_test, y_pred)\n",
    "classification_report_model = classification_report(y_test, y_pred)\n",
    "\n",
    "# Define the file path\n",
    "result_folder = 'C:/Shanila/CSE/cse445/final_project/sha/result3/'\n",
    "result_file_path = result_folder + 'randomized_search_model.txt'\n",
    "\n",
    "# Open the file for writing\n",
    "with open(result_file_path, 'w') as file:\n",
    "    file.write(\"Accuracy Score:\\n\")\n",
    "    file.write(f\"{accuracy_score_model}\\n\\n\")\n",
    "\n",
    "    file.write(\"Classification Report:\\n\")\n",
    "    file.write(f\"{classification_report_model}\\n\")\n",
    "\n",
    "print(f\"Results saved to {result_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': [rf_randomcv.best_params_['criterion']],\n",
    "    'max_depth': [rf_randomcv.best_params_['max_depth']],\n",
    "    'max_features': [rf_randomcv.best_params_['max_features']],\n",
    "    'min_samples_leaf': [rf_randomcv.best_params_['min_samples_leaf'], \n",
    "                         rf_randomcv.best_params_['min_samples_leaf'] + 2, \n",
    "                         rf_randomcv.best_params_['min_samples_leaf'] + 4],\n",
    "    'min_samples_split': [split for split in range(2, 6)],  # Choose valid values within the range\n",
    "    'n_estimators': [rf_randomcv.best_params_['n_estimators'] - 200, rf_randomcv.best_params_['n_estimators'] - 100, \n",
    "                     rf_randomcv.best_params_['n_estimators'], \n",
    "                     rf_randomcv.best_params_['n_estimators'] + 100, rf_randomcv.best_params_['n_estimators'] + 200]\n",
    "}\n",
    "\n",
    "\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Fit the grid_search to the data\n",
    "rf=RandomForestClassifier()\n",
    "grid_search=GridSearchCV(estimator=rf,param_grid=param_grid,cv=10,n_jobs=-1,verbose=2)\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid=grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid.fit(X_train, y_train)\n",
    "y_pred=best_grid.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(\"Accuracy Score {}\".format(accuracy_score(y_test,y_pred)))\n",
    "print(\"Classification report: {}\".format(classification_report(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save grid search result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Access the results from the grid search\n",
    "results = grid_search.cv_results_\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "# Specify the file path for saving the results as a TSV (text) file\n",
    "result_file_path = r'C:\\Shanila\\CSE\\cse445\\final_project\\sha\\result3\\grid_search_results.txt'\n",
    "# Save the results to a TSV file\n",
    "results_df.to_csv(result_file_path, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate graph for grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the 'results' dictionary to a pandas DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Extract relevant data for Mean Test Score and Standard Deviation\n",
    "mean_test_score = results_df['mean_test_score']\n",
    "std_test_score = results_df['std_test_score']\n",
    "\n",
    "# Hyperparameter values\n",
    "n_estimators = results_df['param_n_estimators']\n",
    "max_depth = results_df['param_max_depth']\n",
    "max_features = results_df['param_max_features']\n",
    "min_samples_split = results_df['param_min_samples_split']\n",
    "min_samples_leaf = results_df['param_min_samples_leaf']\n",
    "\n",
    "# Create a scatter plot for Mean Test Score vs. Number of Estimators\n",
    "plt.figure(figsize=(1920/96, 1080/96))  # Set the figure size for HD (1920x1080)\n",
    "plt.scatter(n_estimators, mean_test_score, c=std_test_score, cmap='viridis')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Mean Test Score')\n",
    "plt.title('Mean Test Score vs. Number of Estimators')\n",
    "plt.colorbar(label='Standard Deviation of Test Score')\n",
    "plt.savefig(r'C:\\Shanila\\CSE\\cse445\\final_project\\sha\\result3\\mean_test_score_vs_estimators.png', dpi=96)  # Set the dpi for HD\n",
    "plt.show()\n",
    "\n",
    "# Create a pairplot to visualize the relationships between hyperparameters\n",
    "pairplot = sns.pairplot(results_df, vars=['param_n_estimators', 'param_max_depth', 'param_max_features', 'param_min_samples_split', 'param_min_samples_leaf'], hue='param_criterion')\n",
    "pairplot.fig.set_size_inches(1920/96, 1080/96)  # Set the figure size for HD (1920x1080)\n",
    "pairplot.savefig(r'C:\\Shanila\\CSE\\cse445\\final_project\\sha\\result3\\pairplot.png', dpi=96)  # Set the dpi for HD\n",
    "pairplot.savefig('pairplot.png', dpi=96)\n",
    "plt.show()\n",
    "\n",
    "# Plot the best hyperparameter values\n",
    "best_params = rf_randomcv.best_params_\n",
    "best_mean_test_score = rf_randomcv.best_score_\n",
    "plt.figure(figsize=(1920/96, 1080/96))  # Set the figure size for HD (1920x1080)\n",
    "plt.bar([str(key) for key in best_params.keys()], [str(val) for val in best_params.values()])\n",
    "plt.title(f'Best Hyperparameters (Mean Test Score: {best_mean_test_score:.4f})')\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(r'C:\\Shanila\\CSE\\cse445\\final_project\\sha\\result3\\best_hyperparameters_bar.png', dpi=96)  # Set the dpi for HD\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameter tuning using optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the objective function that Optuna will optimize\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to optimize\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 32)\n",
    "    min_samples_split = trial.suggest_float('min_samples_split', 0.1, 1.0)\n",
    "    min_samples_leaf = trial.suggest_float('min_samples_leaf', 0.1, 0.5)\n",
    "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
    "    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "\n",
    "    # Create the model with the hyperparameters\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        criterion=criterion,\n",
    "        random_state=42  # You can set a random seed for reproducibility\n",
    "    )\n",
    "\n",
    "    # Calculate the cross-validation score (change this according to your dataset)\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    mean_cv_score = cv_scores.mean()\n",
    "\n",
    "    return mean_cv_score\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction='maximize')  # We want to maximize the cross-validation score\n",
    "\n",
    "# Start the optimization process\n",
    "study.optimize(objective, n_trials=100)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Hyperparameters for Each Algorithm Separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy classifier, hyperparameter optimization random search, grid search, baysean optimization and optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.int = int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score\\\n",
    "\n",
    "\n",
    "# Load the training and testing data from CSV files\n",
    "train_data = pd.read_csv(\"C:/Shanila/CSE/cse445/final_project/sha/train/train_data.csv\")\n",
    "test_data = pd.read_csv(\"C:/Shanila/CSE/cse445/final_project/sha/test/test_data.csv\")\n",
    "\n",
    "# Separate features (X) and target (y) for both training and testing data\n",
    "X_train = train_data.drop(columns=[\"Learning disorder\"])\n",
    "y_train = train_data[\"Learning disorder\"]\n",
    "X_test = test_data.drop(columns=[\"Learning disorder\"])\n",
    "y_test = test_data[\"Learning disorder\"]\n",
    "\n",
    "# Define the hyperparameter search space for the Dummy Classifier\n",
    "param_dist_dummy = {\n",
    "    'strategy': ['most_frequent', 'stratified', 'prior', 'uniform']\n",
    "}\n",
    "\n",
    "# Perform hyperparameter optimization for Dummy Classifier using RandomizedSearchCV, GridSearchCV, BayesSearchCV, or Optuna\n",
    "# Random Search\n",
    "dummy_random_search = RandomizedSearchCV(DummyClassifier(), param_distributions=param_dist_dummy, n_iter=10, cv=5, n_jobs=-1)\n",
    "dummy_random_search.fit(X_train, y_train)\n",
    "random_search_accuracy = dummy_random_search.score(X_test, y_test)\n",
    "\n",
    "# Grid Search\n",
    "dummy_grid_search = GridSearchCV(DummyClassifier(), param_grid=param_dist_dummy, cv=5, n_jobs=-1)\n",
    "dummy_grid_search.fit(X_train, y_train)\n",
    "grid_search_accuracy = dummy_grid_search.score(X_test, y_test)\n",
    "\n",
    "# Bayesian Optimization\n",
    "bayesian_opt = BayesSearchCV(DummyClassifier(), search_spaces=param_dist_dummy, n_iter=50, cv=5, n_jobs=-1)\n",
    "bayesian_opt.fit(X_train, y_train)\n",
    "bayesian_opt_accuracy = bayesian_opt.score(X_test, y_test)\n",
    "\n",
    "# Optuna\n",
    "def objective(trial):\n",
    "    strategy = trial.suggest_categorical('strategy', ['most_frequent', 'stratified', 'prior', 'uniform'])\n",
    "    dummy = DummyClassifier(strategy=strategy)\n",
    "    dummy.fit(X_train, y_train)\n",
    "    return dummy.score(X_test, y_test)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "optuna_best_accuracy = study.best_value\n",
    "\n",
    "# Step 5: Save the accuracy results to a text file\n",
    "results_file = r'C:\\Shanila\\CSE\\cse445\\final_project\\sha\\hyperparameter_optimization\\dummy_classifier_results.txt'\n",
    "\n",
    "with open(results_file, 'w') as file:\n",
    "    file.write(f'Random Search Accuracy: {random_search_accuracy}\\n')\n",
    "    file.write(f'Grid Search Accuracy: {grid_search_accuracy}\\n')\n",
    "    file.write(f'Bayesian Optimization Accuracy: {bayesian_opt_accuracy}\\n')\n",
    "    file.write(f'Optuna Best Accuracy: {optuna_best_accuracy}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decision tree classifier, hyperparameter optimization random search, grid search, baysean optimization and optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the training and testing data from CSV files\n",
    "train_data = pd.read_csv(\"C:/Shanila/CSE/cse445/final_project/sha/train/train_data.csv\")\n",
    "test_data = pd.read_csv(\"C:/Shanila/CSE/cse445/final_project/sha/test/test_data.csv\")\n",
    "\n",
    "# Separate features (X) and target (y) for both training and testing data\n",
    "X_train = train_data.drop(columns=[\"Learning disorder\"])\n",
    "y_train = train_data[\"Learning disorder\"]\n",
    "X_test = test_data.drop(columns=[\"Learning disorder\"])\n",
    "y_test = test_data[\"Learning disorder\"]\n",
    "\n",
    "# Define the hyperparameter search space for the Decision Tree\n",
    "param_dist_decision_tree = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Perform hyperparameter optimization for Decision Tree using RandomizedSearchCV, GridSearchCV, BayesSearchCV, or Optuna\n",
    "# Random Search\n",
    "decision_tree_random_search = RandomizedSearchCV(DecisionTreeClassifier(), param_distributions=param_dist_decision_tree, n_iter=10, cv=5, n_jobs=-1)\n",
    "decision_tree_random_search.fit(X_train, y_train)\n",
    "random_search_accuracy = decision_tree_random_search.score(X_test, y_test)\n",
    "\n",
    "# Grid Search\n",
    "decision_tree_grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid=param_dist_decision_tree, cv=5, n_jobs=-1)\n",
    "decision_tree_grid_search.fit(X_train, y_train)\n",
    "grid_search_accuracy = decision_tree_grid_search.score(X_test, y_test)\n",
    "\n",
    "# Bayesian Optimization\n",
    "bayesian_opt = BayesSearchCV(DecisionTreeClassifier(), search_spaces=param_dist_decision_tree, n_iter=50, cv=5, n_jobs=-1)\n",
    "bayesian_opt.fit(X_train, y_train)\n",
    "bayesian_opt_accuracy = bayesian_opt.score(X_test, y_test)\n",
    "\n",
    "# Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
    "        'splitter': trial.suggest_categorical('splitter', ['best', 'random']),\n",
    "        'max_depth': trial.suggest_int('max_depth', 10, 50),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 4)\n",
    "    }\n",
    "    model = DecisionTreeClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.score(X_test, y_test)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "optuna_best_accuracy = study.best_value\n",
    "\n",
    "# Step 5: Save the accuracy results to a text file\n",
    "results_file = r'C:\\Shanila\\CSE\\cse445\\final_project\\sha\\hyperparameter_optimization\\decision_tree_results.txt'\n",
    "\n",
    "with open(results_file, 'w') as file:\n",
    "    file.write(f'Random Search Accuracy: {random_search_accuracy}\\n')\n",
    "    file.write(f'Grid Search Accuracy: {grid_search_accuracy}\\n')\n",
    "    file.write(f'Bayesian Optimization Accuracy: {bayesian_opt_accuracy}\\n')\n",
    "    file.write(f'Optuna Best Accuracy: {optuna_best_accuracy}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "knn classifier, hyperparameter optimization random search, grid search, baysean optimization and optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the training and testing data from CSV files\n",
    "train_data = pd.read_csv(\"C:/Shanila/CSE/cse445/final_project/sha/train/train_data.csv\")\n",
    "test_data = pd.read_csv(\"C:/Shanila/CSE/cse445/final_project/sha/test/test_data.csv\")\n",
    "\n",
    "# Separate features (X) and target (y) for both training and testing data\n",
    "X_train = train_data.drop(columns=[\"Learning disorder\"])\n",
    "y_train = train_data[\"Learning disorder\"]\n",
    "X_test = test_data.drop(columns=[\"Learning disorder\"])\n",
    "y_test = test_data[\"Learning disorder\"]\n",
    "\n",
    "# Define the hyperparameter search space for the KNN Classifier\n",
    "param_dist_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "# Perform hyperparameter optimization for KNN Classifier using RandomizedSearchCV\n",
    "# Random Search\n",
    "knn_random_search = RandomizedSearchCV(KNeighborsClassifier(), param_distributions=param_dist_knn, n_iter=10, cv=5, n_jobs=-1)\n",
    "knn_random_search.fit(X_train, y_train)\n",
    "random_search_accuracy = knn_random_search.score(X_test, y_test)\n",
    "\n",
    "# Grid Search\n",
    "knn_grid_search = GridSearchCV(KNeighborsClassifier(), param_grid=param_dist_knn, cv=5, n_jobs=-1)\n",
    "knn_grid_search.fit(X_train, y_train)\n",
    "grid_search_accuracy = knn_grid_search.score(X_test, y_test)\n",
    "\n",
    "# Bayesian Optimization (BayesSearchCV)\n",
    "bayesian_opt = BayesSearchCV(KNeighborsClassifier(), search_spaces=param_dist_knn, n_iter=50, cv=5, n_jobs=-1)\n",
    "bayesian_opt.fit(X_train, y_train)\n",
    "bayesian_opt_accuracy = bayesian_opt.score(X_test, y_test)\n",
    "\n",
    "# Optuna\n",
    "def objective(trial):\n",
    "    n_neighbors = trial.suggest_int('n_neighbors', 3, 9)\n",
    "    weights = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
    "    p = trial.suggest_categorical('p', [1, 2])\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, p=p)\n",
    "    knn.fit(X_train, y_train)\n",
    "    return knn.score(X_test, y_test)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "optuna_best_accuracy = study.best_value\n",
    "\n",
    "# Step 5: Save the accuracy results to a text file\n",
    "results_file = r'C:\\Shanila\\CSE\\cse445\\final_project\\sha\\hyperparameter_optimization\\knn_classifier_results.txt'\n",
    "\n",
    "with open(results_file, 'w') as file:\n",
    "    file.write(f'Random Search Accuracy: {random_search_accuracy}\\n')\n",
    "    file.write(f'Grid Search Accuracy: {grid_search_accuracy}\\n')\n",
    "    file.write(f'Bayesian Optimization Accuracy: {bayesian_opt_accuracy}\\n')\n",
    "    file.write(f'Optuna Best Accuracy: {optuna_best_accuracy}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression, hyperparameter optimization random search, grid search, baysean optimization and optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score\n",
    "import traceback\n",
    "\n",
    "# Load the training and testing data from CSV files\n",
    "train_data = pd.read_csv(\"C:/Shanila/CSE/cse445/final_project/sha/train/train_data.csv\")\n",
    "test_data = pd.read_csv(\"C:/Shanila/CSE/cse445/final_project/sha/test/test_data.csv\")\n",
    "\n",
    "# Separate features (X) and target (y) for both training and testing data\n",
    "X_train = train_data.drop(columns=[\"Learning disorder\"])\n",
    "y_train = train_data[\"Learning disorder\"]\n",
    "X_test = test_data.drop(columns=[\"Learning disorder\"])\n",
    "y_test = test_data[\"Learning disorder\"]\n",
    "\n",
    "# Define the hyperparameter search space for Logistic Regression\n",
    "param_dist_logistic = {\n",
    "    'penalty': ['l2'],  # Use 'l2' penalty for 'lbfgs' solver\n",
    "    'C': [0.001, 0.01, 0.1, 1.0, 10.0],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga'],\n",
    "    'max_iter': [100, 1000, 10000]\n",
    "}\n",
    "\n",
    "# Random Search\n",
    "logistic_random_search = RandomizedSearchCV(LogisticRegression(), param_distributions=param_dist_logistic, n_iter=10, cv=5, n_jobs=-1)\n",
    "logistic_random_search.fit(X_train, y_train)\n",
    "random_search_accuracy = logistic_random_search.score(X_test, y_test)\n",
    "\n",
    "# Grid Search\n",
    "logistic_grid_search = GridSearchCV(LogisticRegression(), param_grid=param_dist_logistic, cv=5, n_jobs=-1)\n",
    "logistic_grid_search.fit(X_train, y_train)\n",
    "grid_search_accuracy = logistic_grid_search.score(X_test, y_test)\n",
    "\n",
    "# Bayesian Optimization\n",
    "bayesian_opt = BayesSearchCV(LogisticRegression(), search_spaces=param_dist_logistic, n_iter=50, cv=5, n_jobs=-1)\n",
    "bayesian_opt.fit(X_train, y_train)\n",
    "bayesian_opt_accuracy = bayesian_opt.score(X_test, y_test)\n",
    "\n",
    "# Optuna\n",
    "def objective(trial):\n",
    "    try:\n",
    "        penalty = 'l2'  # Use 'l2' penalty for 'lbfgs' solver\n",
    "        C = trial.suggest_float('C', 1e-5, 1e5, log=True)\n",
    "        solver = trial.suggest_categorical('solver', ['newton-cg', 'lbfgs', 'liblinear', 'saga'])\n",
    "        max_iter = trial.suggest_int('max_iter', 100, 10000)\n",
    "\n",
    "        logistic = LogisticRegression(penalty=penalty, C=C, solver=solver, max_iter=max_iter)\n",
    "        logistic.fit(X_train, y_train)\n",
    "        return logistic.score(X_test, y_test)\n",
    "    except Exception as e:\n",
    "        print(f\"Trial failed: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return float('nan')\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "optuna_best_accuracy = study.best_value\n",
    "\n",
    "# Step 5: Save the accuracy results to a text file\n",
    "results_file = r'C:\\Shanila\\CSE\\cse445\\final_project\\sha\\hyperparameter_optimization\\logistic_regression_results.txt'\n",
    "\n",
    "with open(results_file, 'w') as file:\n",
    "    file.write(f'Random Search Accuracy: {random_search_accuracy}\\n')\n",
    "    file.write(f'Grid Search Accuracy: {grid_search_accuracy}\\n')\n",
    "    file.write(f'Bayesian Optimization Accuracy: {bayesian_opt_accuracy}\\n')\n",
    "    file.write(f'Optuna Best Accuracy: {optuna_best_accuracy}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest, hyperparameter optimization random search, grid search, baysean optimization and optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "15 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [0.98966489 0.98902182        nan        nan 0.98901973 0.98902182\n",
      " 0.98837457 0.98902182 0.98966489        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "2700 fits failed out of a total of 5400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "495 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2205 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan ... 0.98902182 0.98902182 0.98902182]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 428, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n    estimator._validate_params()\n  File \"c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n    validate_parameter_constraints(\n  File \"c:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32mc:\\Shanila\\CSE\\cse445\\final_project\\sha\\Learning_disorder.ipynb Cell 90\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Shanila/CSE/cse445/final_project/sha/Learning_disorder.ipynb#Y223sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# Bayesian Optimization\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Shanila/CSE/cse445/final_project/sha/Learning_disorder.ipynb#Y223sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m bayesian_opt \u001b[39m=\u001b[39m BayesSearchCV(RandomForestClassifier(), search_spaces\u001b[39m=\u001b[39mparam_dist_rf, n_iter\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Shanila/CSE/cse445/final_project/sha/Learning_disorder.ipynb#Y223sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m bayesian_opt\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Shanila/CSE/cse445/final_project/sha/Learning_disorder.ipynb#Y223sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m bayesian_opt_accuracy \u001b[39m=\u001b[39m bayesian_opt\u001b[39m.\u001b[39mscore(X_test, y_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Shanila/CSE/cse445/final_project/sha/Learning_disorder.ipynb#Y223sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# Optuna\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\skopt\\searchcv.py:466\u001b[0m, in \u001b[0;36mBayesSearchCV.fit\u001b[1;34m(self, X, y, groups, callback, **fit_params)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_kwargs_ \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_kwargs)\n\u001b[1;32m--> 466\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mfit(X\u001b[39m=\u001b[39mX, y\u001b[39m=\u001b[39my, groups\u001b[39m=\u001b[39mgroups, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    468\u001b[0m \u001b[39m# BaseSearchCV never ranked train scores,\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39m# but apparently we used to ship this (back-compat)\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_train_score:\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\skopt\\searchcv.py:512\u001b[0m, in \u001b[0;36mBayesSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[39mwhile\u001b[39;00m n_iter \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    509\u001b[0m     \u001b[39m# when n_iter < n_points points left for evaluation\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     n_points_adjusted \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(n_iter, n_points)\n\u001b[1;32m--> 512\u001b[0m     optim_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step(\n\u001b[0;32m    513\u001b[0m         search_space, optimizer,\n\u001b[0;32m    514\u001b[0m         evaluate_candidates, n_points\u001b[39m=\u001b[39mn_points_adjusted\n\u001b[0;32m    515\u001b[0m     )\n\u001b[0;32m    516\u001b[0m     n_iter \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m n_points\n\u001b[0;32m    518\u001b[0m     \u001b[39mif\u001b[39;00m eval_callbacks(callbacks, optim_result):\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\skopt\\searchcv.py:408\u001b[0m, in \u001b[0;36mBayesSearchCV._step\u001b[1;34m(self, search_space, optimizer, evaluate_candidates, n_points)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[39m# make lists into dictionaries\u001b[39;00m\n\u001b[0;32m    406\u001b[0m params_dict \u001b[39m=\u001b[39m [point_asdict(search_space, p) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m params]\n\u001b[1;32m--> 408\u001b[0m all_results \u001b[39m=\u001b[39m evaluate_candidates(params_dict)\n\u001b[0;32m    409\u001b[0m \u001b[39m# Feed the point and objective value back into optimizer\u001b[39;00m\n\u001b[0;32m    410\u001b[0m \u001b[39m# Optimizer minimizes objective, hence provide negative score\u001b[39;00m\n\u001b[0;32m    411\u001b[0m local_results \u001b[39m=\u001b[39m all_results[\u001b[39m\"\u001b[39m\u001b[39mmean_test_score\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(params):]\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[39m=\u001b[39mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[39m=\u001b[39mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[39m=\u001b[39mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[39m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[39m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[39mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[39menumerate\u001b[39m(candidate_params), \u001b[39menumerate\u001b[39m(cv\u001b[39m.\u001b[39msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget(timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39mresult(timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m    457\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score\n",
    "import traceback\n",
    "\n",
    "# Load the training and testing data from CSV files\n",
    "train_data = pd.read_csv(\"C:/Shanila/CSE/cse445/final_project/sha/train/train_data.csv\")\n",
    "test_data = pd.read_csv(\"C:/Shanila/CSE/cse445/final_project/sha/test/test_data.csv\")\n",
    "\n",
    "# Separate features (X) and target (y) for both training and testing data\n",
    "X_train = train_data.drop(columns=[\"Learning disorder\"])\n",
    "y_train = train_data[\"Learning disorder\"]\n",
    "X_test = test_data.drop(columns=[\"Learning disorder\"])\n",
    "y_test = test_data[\"Learning disorder\"]\n",
    "\n",
    "# Define the hyperparameter search space for Random Forest\n",
    "param_dist_rf = {\n",
    "    'n_estimators': [100, 300, 500, 800, 1000],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Random Search\n",
    "rf_random_search = RandomizedSearchCV(RandomForestClassifier(), param_distributions=param_dist_rf, n_iter=10, cv=5, n_jobs=-1)\n",
    "rf_random_search.fit(X_train, y_train)\n",
    "random_search_accuracy = rf_random_search.score(X_test, y_test)\n",
    "\n",
    "# Grid Search\n",
    "rf_grid_search = GridSearchCV(RandomForestClassifier(), param_grid=param_dist_rf, cv=5, n_jobs=-1)\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "grid_search_accuracy = rf_grid_search.score(X_test, y_test)\n",
    "\n",
    "# Bayesian Optimization\n",
    "bayesian_opt = BayesSearchCV(RandomForestClassifier(), search_spaces=param_dist_rf, n_iter=50, cv=5, n_jobs=-1)\n",
    "bayesian_opt.fit(X_train, y_train)\n",
    "bayesian_opt_accuracy = bayesian_opt.score(X_test, y_test)\n",
    "\n",
    "# Optuna\n",
    "def objective(trial):\n",
    "    try:\n",
    "        n_estimators = trial.suggest_int('n_estimators', 100, 1000, log=True)\n",
    "        max_depth = trial.suggest_int('max_depth', 1, 32)\n",
    "        min_samples_split = trial.suggest_float('min_samples_split', 0.1, 1)\n",
    "        min_samples_leaf = trial.suggest_float('min_samples_leaf', 0.1, 0.5)\n",
    "        max_features = trial.suggest_categorical('max_features', ['auto', 'sqrt'])\n",
    "        bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
    "\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            bootstrap=bootstrap\n",
    "        )\n",
    "\n",
    "        rf.fit(X_train, y_train)\n",
    "        return rf.score(X_test, y_test)\n",
    "    except Exception as e:\n",
    "        print(f\"Trial failed: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return float('nan')\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "optuna_best_accuracy = study.best_value\n",
    "\n",
    "# Step 5: Save the accuracy results to a text file\n",
    "results_file = r'C:\\Shanila\\CSE\\cse445\\final_project\\sha\\hyperparameter_optimization\\random_forest_results.txt'\n",
    "\n",
    "with open(results_file, 'w') as file:\n",
    "    file.write(f'Random Search Accuracy: {random_search_accuracy}\\n')\n",
    "    file.write(f'Grid Search Accuracy: {grid_search_accuracy}\\n')\n",
    "    file.write(f'Bayesian Optimization Accuracy: {bayesian_opt_accuracy}\\n')\n",
    "    file.write(f'Optuna Best Accuracy: {optuna_best_accuracy}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
